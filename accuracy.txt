Improving Requirement Extraction Accuracy in PropelAI
Text-Cleaning Pipeline Improvements
•	Robust PDF/text extraction: Use specialized parsers and OCR to handle complex layouts. For text-based PDFs, libraries like Camelot, Tabula or pdfplumber can reliably extract table contents into structured textunstract.com. For scanned or image-only PDFs, apply a high-quality OCR (e.g. Tesseract) and detect rotated pages, since embedded text can be misaligned or missingunstract.comunstract.com.
•	Normalize formatting: Strip out headers/footers and footnotes that can confuse parsers. Merge broken lines by removing soft hyphens and joining line-wrapped sentences. Collapse bullet or numbered lists into sentences (e.g. remove “•” or “1.” markers) so that requirement clauses aren’t split across lines. Convert multiple spaces and non-standard unicode to plain text.
•	Flatten tables and lists: If requirements appear inside tables or multi-column layouts, ensure cells are concatenated with delimiters (tabs or spaces) before NLP. As [18] notes, preserving table layout helps text models interpret contentunstract.com. For example, use a table-extraction library to convert a PDF table row into a line of text with fields separated by tabs or pipes.
•	Handle artifacts: Remove pagination and decorative text. If the PDF contains a hidden OCR layer (common in scanned documents), detect low OCR quality as a cue to re-run OCRunstract.com. For broken OCR output (e.g. double-spaced characters), apply post-OCR cleaning such as de-hyphenation and merging character fragments.
Expanded and Restructured Requirement Dictionary
•	Broaden keyword coverage: Include more verb forms and synonyms of requirement verbs. For example, add entries like deliver*, supply, complete, support, etc., possibly using a thesaurus (WordNet) to seed related terms. The existing dictionary (e.g. VisibleThread style) should recognize not just base forms but also noun/verb variations and multi-word patterns (e.g. “Offeror shall provide” vs. “provide”).
•	Contextual patterns: Create composite dictionary entries for phrases that signal requirements only in context. For instance, treat “shall provide” as stronger evidence than “provide” alone. This could mean having separate rules like Offeror shall provide (exact match) vs. a wildcard * provide that only triggers in Section C.
•	Category labels: Organize dictionary terms by requirement type or strength (mandatory vs. recommended). For example, tag must and shall as mandatory, while should and encourage as desirable. This aligns with best practices: terms like “shall”/“must” typically indicate obligations, whereas “should” may not. Use these categories later for confidence scoring.
•	Hierarchical dictionary: Consider a nested structure (VisibleThread style) where high-level concepts map to sub-terms. For example, a Documentation category might include manual, report, documentation, each flagged as requirement keywords. This aids in grouping and future expansion.
•	Regular updates: Analyze past extractions to find frequently missed terms. Update the dictionary iteratively based on errors. For example, if “provide” is often mis-tagged, add context rules or synonyms (e.g. “furnish”) to reduce ambiguity. This dictionary-driven approach reflects the system’s existing design but with more coverage and structure.
Hybrid Regex and ML Pattern Detection
•	Rule-based (regex) components: Use regular expressions to catch well-defined patterns: numbered clauses (e.g. \b\d+\.\d+(\.\d+)*), dates, section headers, or phrases like “if … then”. As [29] observes, regex is “efficient and precise” for fixed patternsml6.eu. For example, a regex for “shall [\w\s]+” can flag mandatory statements. These rules can run first to filter obvious requirements.
•	Machine-learning classifiers: Train a lightweight text classifier (e.g. using scikit-learn or spaCy) to label sentences as “requirement” vs. “not requirement.” Use features like keywords from the dictionary, sentence position, and part-of-speech patterns. Alternatively, fine-tune an open-source transformer (e.g. RoBERTa) on a labeled corpus of RFP text. ML is useful for context-driven cases where simple rules failml6.eu.
•	Ensemble approach: Combine rule and ML outputs. For example, label a sentence as a requirement if either regex matches a high-priority pattern or the ML model score exceeds a threshold. Use majority voting or weighted scores if multiple detectors are applied. As noted in hybrid NLP designs, rules can “pre-filter or post-process outputs” to improve accuracyml6.eu, while ML handles edge cases. For instance, apply regex to catch “must” or “shall” phrases first, then let ML classify remaining sentences.
•	Dynamic pattern generation: (Optional) Employ tools like Semgrep or machine-learning generated rules to find new patterns. For example, analyze false negatives to derive new regex patterns. Over time, this closes gaps in the rule set.
Section-Aware Parsing
•	Segment by solicitation section: First divide the document into RFP sections (e.g. Sections C, L, M, and attachments) based on headings or known markers (e.g. “SECTION C – The Contract”). Process each section separately. For example, treat text under Section C (statement of work) with higher sensitivity to requirements. In Section L (proposal instructions), similar phrasing often means “action for the offeror” rather than a contract requirement.
•	Section-specific rules/dictionaries: Maintain separate keyword lists or scoring adjustments per section. For instance, the verb “submit” in Section L (where instructions like “submit a proposal” appear) should not trigger a requirement, whereas “submit” in an evaluation form might. Tailor wildcard patterns so that provid* in Section L only matches patterns like “shall provide”, not standalone “provide”. This leverages the context of where the text appears, as RFP structure dictates different intent in C vs L vs M.
•	Header detection: Use regex or PDF structure to detect section headers and annotate content blocks. Tag each sentence with its section label (C/L/M) before extraction. Then, require matches to also satisfy section-appropriate context. For example, only consider “mandatory attachments” mentions as requirements if in Section L/M, not in Section C.
•	Priority by section: If confidence is borderline, but the sentence falls under a critical section (e.g. C or M), lean towards marking it as a requirement. Conversely, be more conservative in the proposal instructions section. This reflects the fact that explicit requirements are usually concentrated in Sections C (PWS/SOW) and L/M (evaluations).
Confidence Scoring and Ambiguity Handling
•	Assign confidences: Each extracted candidate should carry a confidence score. For ML models, use the prediction probability. For rule matches, define heuristic confidences (e.g. 0.9 for a match on “shall”, 0.7 on “should”). Combine scores if multiple rules apply. A simple approach is to take the maximum or an average of contributing scores.
•	Threshold tuning: Choose a cutoff to decide which items are automatically accepted vs. flagged. As seen in machine learning practice, raising the confidence threshold reduces false positives (improves precision) but can lower recallmindee.com. Plotting a precision–recall curve on validation data helps pick an optimal tradeoffmindee.com. For instance, if missing a requirement is costlier than catching an extra false alarm, set a lower threshold to boost recall.
•	Fallback flag (“Review Needed”): If a candidate’s confidence falls below the threshold, label it as ambiguous. In the UI or output matrix, mark these as “Review Needed” so a user can manually verify. This human-in-the-loop safety net ensures that unclear cases aren’t silently dropped. For example, a sentence scoring 0.6 (below a 0.7 threshold) would appear in the output marked for review. This aligns with standard ML practice of deferring low-confidence decisions to manual reviewmindee.com.
•	Combine sources of uncertainty: If using both regex and ML, consider reducing confidence when they disagree. E.g. if regex flags a requirement but ML says “no”, downweight that item. Track confidence based on the degree of agreement.
Evaluation and Benchmarking Methods
•	Ground-truth dataset: Prepare a validation set of RFPs with manually annotated requirements (both explicit and implicit). Ensure it covers varied formats (tables, lists, scanned text). Split into development (for tuning) and hold-out test sets.
•	Metrics – Precision/Recall/F1: Measure extraction performance using precision (fraction of extracted items that are true requirements) and recall (fraction of true requirements that were extracted)learn.microsoft.com. The F1-score balances them. For example, a model that finds 80/100 actual requirements with 90 total extractions has 80% recall and 89% precision. Use these to compare pipeline versions.
•	Per-section evaluation: Compute metrics separately for each RFP section and for different categories of requirements. This highlights, e.g., if Section L instructions cause many false positives.
•	PR curve analysis: Vary the confidence threshold and plot precision vs recallmindee.com. Choose a threshold that gives an acceptable balance for your use case (e.g. maximize F1 or favor higher recall).
•	Error analysis: Review common false negatives (missed requirements) and false positives. This may reveal patterns to add to the dictionary or adjust rules. For instance, if all missed cases involve the word “provide” at clause starts, enhance the dictionary or regex to catch those.
•	Iterative benchmarking: Each time changes are made (e.g. new regex or dictionary term), re-evaluate to ensure overall improvement. Track metrics over time so the solo developer can verify gains without bias.
Sources: Best practices in NLP extraction and compliance analysis support these methodsml6.euml6.eumindee.comlearn.microsoft.com. These references detail the use of keyword-based extraction in RFPs, the benefits of rule/ML hybrids, and standard evaluation metrics (precision, recall) for validation.

