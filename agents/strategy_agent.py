"""
PropelAI Strategy Agent - "The Capture Manager"
Play 2: The Strategy Engine (Win Themes)

v4.0: Enhanced with real LLM integration for strategic analysis

Goal: Define *why* we win before writing a single word

This agent:
1. Analyzes Section M (Evaluation Factors) using LLM reasoning
2. Queries past bid strategies from the Agent-Trace database
3. Performs competitor ghosting analysis
4. Generates win themes and discriminators via LLM
5. Creates annotated outline with page allocations

Uses high-reasoning model (Gemini 1.5 Pro or Claude) for strategic synthesis
"""

import json
import re
import os
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False

from core.state import ProposalState, ProposalPhase


# LLM Provider configuration
LLM_PROVIDERS = {
    "gemini": {
        "base_url": "https://generativelanguage.googleapis.com/v1beta",
        "model": "gemini-1.5-pro",
        "env_key": "GOOGLE_API_KEY",
    },
    "anthropic": {
        "base_url": "https://api.anthropic.com/v1",
        "model": "claude-3-5-sonnet-20241022",
        "env_key": "ANTHROPIC_API_KEY",
    },
    "openai": {
        "base_url": "https://api.openai.com/v1",
        "model": "gpt-4-turbo-preview",
        "env_key": "OPENAI_API_KEY",
    },
}


class StrategyFramework(str, Enum):
    """Common proposal strategy frameworks"""
    PRICE_TO_WIN = "price_to_win"
    TECHNICAL_EXCELLENCE = "technical_excellence"
    PAST_PERFORMANCE = "past_performance"
    INNOVATION = "innovation"
    RISK_MITIGATION = "risk_mitigation"
    PARTNERSHIP = "partnership"
    TRANSITION = "transition"


class DiscriminatorType(str, Enum):
    """Categories of discriminators"""
    TECHNICAL = "technical"
    MANAGEMENT = "management"
    PAST_PERFORMANCE = "past_performance"
    PRICE = "price"
    INNOVATION = "innovation"
    RISK = "risk"
    PERSONNEL = "personnel"


@dataclass
class Discriminator:
    """
    v4.0 Iron Triangle: A unique differentiator that sets us apart.

    Each discriminator should be:
    - Provable with evidence
    - Relevant to evaluation criteria
    - Difficult for competitors to match
    """
    id: str
    category: DiscriminatorType
    claim: str                         # "Our patented AI reduces review time by 40%"
    evidence_type: str                 # "case_study" | "metric" | "testimonial" | "certification"
    evidence_source: Optional[str]     # Document reference
    quantified_impact: Optional[str]   # "40% faster" | "$2M saved"
    ghosting_angle: Optional[str]      # How this positions against competitors


@dataclass
class WinTheme:
    """
    v4.0 Iron Triangle: A strategic win theme with full evidence chain.

    Each theme addresses a primary evaluation factor with:
    - Clear headline message
    - Multiple discriminators
    - Provable evidence points
    - Competitive positioning
    """
    id: str
    theme_headline: str                # The headline message (1 sentence)
    theme_narrative: str               # 2-3 sentence expansion
    discriminators: List[Discriminator] = field(default_factory=list)
    proof_points: List[str] = field(default_factory=list)
    linked_eval_criteria: List[str] = field(default_factory=list)
    ghosting_language: Optional[str] = None
    page_allocation_percent: float = 0.0  # Suggested % of volume
    priority: int = 1                  # 1-5 ranking (1 = highest)
    confidence: float = 0.5
    llm_generated: bool = False        # True if generated by LLM vs template

    # Legacy compatibility
    @property
    def theme_text(self) -> str:
        return self.theme_headline

    @property
    def discriminator(self) -> str:
        if self.discriminators:
            return self.discriminators[0].claim
        return ""

    @property
    def linked_criteria(self) -> List[str]:
        return self.linked_eval_criteria


@dataclass
class CompetitorProfile:
    """Intelligence on a competitor"""
    name: str
    strengths: List[str] = field(default_factory=list)
    weaknesses: List[str] = field(default_factory=list)
    likely_themes: List[str] = field(default_factory=list)
    ghosting_opportunities: List[str] = field(default_factory=list)


@dataclass
class GhostingStrategy:
    """
    v4.0: Subtle competitive positioning language.

    Ghosting is about positioning ourselves favorably without
    directly naming competitors.
    """
    competitor_weakness: str           # What competitor lacks
    our_strength: str                  # Our counter-positioning
    language_template: str             # "Unlike solutions that..., our approach..."
    eval_criteria_link: str            # Which Section M factor this addresses
    subtlety_level: int = 3            # 1-5 (1=very subtle, 5=direct comparison)


class StrategyAgent:
    """
    The Strategy Agent - "The Capture Manager"
    
    Specialized in:
    - Evaluation factor analysis (Section M)
    - Win theme development
    - Competitor ghosting
    - Storyboarding and outline generation
    """
    
    def __init__(
        self,
        llm_client: Optional[Any] = None,
        past_performance_store: Optional[Any] = None,
        llm_provider: str = "gemini",
        use_llm: bool = True
    ):
        """
        Initialize the Strategy Agent

        Args:
            llm_client: LLM client for strategic reasoning (recommend Gemini Pro)
            past_performance_store: Vector store of past proposals for pattern matching
            llm_provider: Which LLM provider to use ("gemini", "anthropic", "openai")
            use_llm: If False, falls back to template-based generation
        """
        self.llm_client = llm_client
        self.past_performance_store = past_performance_store
        self.llm_provider = llm_provider
        self.use_llm = use_llm and HTTPX_AVAILABLE
        self._http_client = None

    def _get_http_client(self) -> Optional["httpx.Client"]:
        """Get or create HTTP client for LLM API calls"""
        if not HTTPX_AVAILABLE:
            return None
        if self._http_client is None:
            self._http_client = httpx.Client(timeout=60.0)
        return self._http_client

    def _call_llm(self, prompt: str, system_prompt: Optional[str] = None) -> Optional[str]:
        """
        Call the LLM API for strategic reasoning.

        v4.0: Real LLM integration replacing template stubs.

        Args:
            prompt: The user prompt
            system_prompt: Optional system instructions

        Returns:
            LLM response text, or None if call fails
        """
        if not self.use_llm:
            return None

        client = self._get_http_client()
        if not client:
            return None

        provider_config = LLM_PROVIDERS.get(self.llm_provider, LLM_PROVIDERS["gemini"])
        api_key = os.environ.get(provider_config["env_key"])

        if not api_key:
            # Try fallback providers
            for fallback in ["gemini", "anthropic", "openai"]:
                if fallback == self.llm_provider:
                    continue
                fallback_config = LLM_PROVIDERS[fallback]
                api_key = os.environ.get(fallback_config["env_key"])
                if api_key:
                    provider_config = fallback_config
                    break

        if not api_key:
            return None

        try:
            if "gemini" in provider_config["model"]:
                return self._call_gemini(client, api_key, prompt, system_prompt, provider_config)
            elif "claude" in provider_config["model"]:
                return self._call_anthropic(client, api_key, prompt, system_prompt, provider_config)
            else:
                return self._call_openai(client, api_key, prompt, system_prompt, provider_config)
        except Exception as e:
            print(f"LLM call failed: {e}")
            return None

    def _call_gemini(
        self,
        client: "httpx.Client",
        api_key: str,
        prompt: str,
        system_prompt: Optional[str],
        config: Dict
    ) -> Optional[str]:
        """Call Gemini API"""
        url = f"{config['base_url']}/models/{config['model']}:generateContent?key={api_key}"

        contents = []
        if system_prompt:
            contents.append({"role": "user", "parts": [{"text": system_prompt}]})
            contents.append({"role": "model", "parts": [{"text": "Understood. I will follow these instructions."}]})
        contents.append({"role": "user", "parts": [{"text": prompt}]})

        response = client.post(
            url,
            json={
                "contents": contents,
                "generationConfig": {
                    "temperature": 0.7,
                    "maxOutputTokens": 4096,
                }
            }
        )

        if response.status_code == 200:
            data = response.json()
            candidates = data.get("candidates", [])
            if candidates:
                content = candidates[0].get("content", {})
                parts = content.get("parts", [])
                if parts:
                    return parts[0].get("text", "")
        return None

    def _call_anthropic(
        self,
        client: "httpx.Client",
        api_key: str,
        prompt: str,
        system_prompt: Optional[str],
        config: Dict
    ) -> Optional[str]:
        """Call Anthropic API"""
        url = f"{config['base_url']}/messages"

        headers = {
            "x-api-key": api_key,
            "anthropic-version": "2023-06-01",
            "content-type": "application/json",
        }

        body = {
            "model": config["model"],
            "max_tokens": 4096,
            "messages": [{"role": "user", "content": prompt}],
        }

        if system_prompt:
            body["system"] = system_prompt

        response = client.post(url, headers=headers, json=body)

        if response.status_code == 200:
            data = response.json()
            content = data.get("content", [])
            if content:
                return content[0].get("text", "")
        return None

    def _call_openai(
        self,
        client: "httpx.Client",
        api_key: str,
        prompt: str,
        system_prompt: Optional[str],
        config: Dict
    ) -> Optional[str]:
        """Call OpenAI API"""
        url = f"{config['base_url']}/chat/completions"

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        response = client.post(
            url,
            headers=headers,
            json={
                "model": config["model"],
                "messages": messages,
                "max_tokens": 4096,
                "temperature": 0.7,
            }
        )

        if response.status_code == 200:
            data = response.json()
            choices = data.get("choices", [])
            if choices:
                return choices[0].get("message", {}).get("content", "")
        return None
        
    def __call__(self, state: ProposalState) -> Dict[str, Any]:
        """
        Main entry point - called by the Orchestrator
        
        Develops win strategy based on Section M and competitive landscape
        """
        start_time = datetime.now()
        
        # Get inputs from state
        evaluation_criteria = state.get("evaluation_criteria", [])
        requirements = state.get("requirements", [])
        instructions = state.get("instructions", [])
        rfp_metadata = state.get("rfp_metadata", {})
        
        if not evaluation_criteria:
            return {
                "error_state": "No evaluation criteria found - run compliance shred first",
                "agent_trace_log": [{
                    "timestamp": start_time.isoformat(),
                    "agent_name": "strategy_agent",
                    "action": "develop_strategy",
                    "input_summary": "Missing evaluation criteria",
                    "output_summary": "Error: Prerequisites not met",
                    "reasoning_trace": "Strategy requires Section M analysis from compliance shred"
                }]
            }
        
        # Phase 1: Analyze evaluation factors
        factor_analysis = self._analyze_evaluation_factors(evaluation_criteria)
        
        # Phase 2: Query past performance for winning patterns
        winning_patterns = self._query_winning_patterns(
            factor_analysis,
            rfp_metadata
        )
        
        # Phase 3: Develop win themes
        win_themes = self._develop_win_themes(
            factor_analysis,
            winning_patterns,
            requirements
        )
        
        # Phase 4: Competitor analysis (if data available)
        competitor_analysis = self._analyze_competitors(
            state.get("competitor_analysis", {}),
            factor_analysis
        )
        
        # Phase 5: Generate ghosting language
        for theme in win_themes:
            theme.ghosting_language = self._generate_ghosting(
                theme,
                competitor_analysis
            )
        
        # Phase 6: Create annotated outline with page allocations
        annotated_outline = self._create_annotated_outline(
            win_themes,
            requirements,
            instructions
        )
        
        # Calculate processing time
        duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        
        # Build trace log
        trace_log = {
            "timestamp": start_time.isoformat(),
            "agent_name": "strategy_agent",
            "action": "develop_strategy",
            "input_summary": f"{len(evaluation_criteria)} eval criteria, {len(requirements)} requirements",
            "output_summary": f"Generated {len(win_themes)} win themes, {len(annotated_outline.get('volumes', {}))} volumes",
            "reasoning_trace": f"Primary factors: {', '.join(f['factor_name'] for f in factor_analysis[:3])}. "
                             f"Strategy framework: {self._determine_primary_framework(factor_analysis)}",
            "duration_ms": duration_ms,
            "tool_calls": [
                {"tool": "analyze_factors", "result": f"{len(factor_analysis)} factors analyzed"},
                {"tool": "query_patterns", "result": f"{len(winning_patterns)} patterns found"},
                {"tool": "develop_themes", "result": f"{len(win_themes)} themes generated"},
            ]
        }
        
        return {
            "current_phase": ProposalPhase.STRATEGY.value,
            "win_themes": [self._win_theme_to_dict(t) for t in win_themes],
            "competitor_analysis": competitor_analysis,
            "annotated_outline": annotated_outline,
            "agent_trace_log": [trace_log],
            "updated_at": datetime.now().isoformat()
        }
    
    def _analyze_evaluation_factors(
        self, 
        criteria: List[Dict]
    ) -> List[Dict[str, Any]]:
        """
        Analyze Section M evaluation factors to understand priorities
        
        Returns ranked list of factors with their relative importance
        """
        analyzed = []
        
        # Score importance based on weight and language
        for criterion in criteria:
            importance_score = 0.5  # Base score
            
            # Adjust for explicit weight
            if criterion.get("weight"):
                importance_score = criterion["weight"]
            
            # Adjust for language signals
            text_lower = criterion.get("text", "").lower()
            
            if "most important" in text_lower or "highest priority" in text_lower:
                importance_score = max(importance_score, 0.4)
            if "critical" in text_lower or "essential" in text_lower:
                importance_score += 0.1
            if "tradeoff" in text_lower:
                importance_score += 0.05  # Tradeoff factors are decision points
            
            analysis = {
                "criterion_id": criterion.get("id"),
                "factor_name": criterion.get("factor_name", "Unknown"),
                "text": criterion.get("text", ""),
                "importance_score": min(importance_score, 1.0),
                "key_phrases": self._extract_key_phrases(criterion.get("text", "")),
                "recommended_emphasis": self._recommend_emphasis(importance_score)
            }
            analyzed.append(analysis)
        
        # Sort by importance
        analyzed.sort(key=lambda x: x["importance_score"], reverse=True)
        
        return analyzed
    
    def _extract_key_phrases(self, text: str) -> List[str]:
        """Extract key phrases that evaluators will look for"""
        import re
        
        # Look for quoted requirements
        quoted = re.findall(r'"([^"]+)"', text)
        
        # Look for emphasized terms
        emphasized = []
        emphasis_patterns = [
            r'\b(must demonstrate|shall provide|is required)\b',
            r'\b(innovative|proven|comprehensive|detailed)\b',
            r'\b(risk mitigation|quality assurance|continuous improvement)\b'
        ]
        
        for pattern in emphasis_patterns:
            matches = re.findall(pattern, text.lower())
            emphasized.extend(matches)
        
        return list(set(quoted + emphasized))[:10]
    
    def _recommend_emphasis(self, importance_score: float) -> str:
        """Recommend level of emphasis in proposal"""
        if importance_score >= 0.35:
            return "PRIMARY - Dedicate significant page count and strongest evidence"
        elif importance_score >= 0.25:
            return "SECONDARY - Strong coverage required"
        elif importance_score >= 0.15:
            return "SUPPORTING - Adequate coverage with clear compliance"
        else:
            return "MINIMAL - Meet requirements, don't over-invest"
    
    def _query_winning_patterns(
        self, 
        factor_analysis: List[Dict],
        rfp_metadata: Dict
    ) -> List[Dict[str, Any]]:
        """
        Query the past performance store for winning patterns
        
        This is where the Data Flywheel kicks in - we learn from past wins
        """
        patterns = []
        
        # If we have a past performance store, query it
        if self.past_performance_store:
            # Query for similar opportunities
            query_text = " ".join([f["factor_name"] for f in factor_analysis])
            # results = self.past_performance_store.similarity_search(query_text)
            # patterns = self._extract_patterns(results)
            pass
        
        # Default patterns based on factor analysis
        primary_factors = [f["factor_name"] for f in factor_analysis[:3]]
        
        if "Technical Approach" in primary_factors:
            patterns.append({
                "pattern_name": "Technical Depth Strategy",
                "description": "Lead with technical innovation and detailed methodology",
                "success_rate": 0.72,
                "key_elements": [
                    "Detailed technical approach with diagrams",
                    "Innovation features highlighted",
                    "Risk identification with mitigation"
                ]
            })
        
        if "Past Performance" in primary_factors:
            patterns.append({
                "pattern_name": "Relevance Mapping Strategy",
                "description": "Map past contracts directly to current requirements",
                "success_rate": 0.68,
                "key_elements": [
                    "Direct requirement-to-experience mapping",
                    "Quantified results and metrics",
                    "Customer references ready"
                ]
            })
        
        if "Management Approach" in primary_factors:
            patterns.append({
                "pattern_name": "Transition Risk Mitigation",
                "description": "Emphasize seamless transition and execution certainty",
                "success_rate": 0.65,
                "key_elements": [
                    "Detailed transition plan",
                    "Key personnel committed",
                    "QA/QC processes documented"
                ]
            })
        
        if "Price/Cost" in primary_factors:
            patterns.append({
                "pattern_name": "Best Value Positioning",
                "description": "Demonstrate value, not just low price",
                "success_rate": 0.58,
                "key_elements": [
                    "Total cost of ownership analysis",
                    "Efficiency gains quantified",
                    "Investment in contract startup"
                ]
            })
        
        return patterns
    
    def _develop_win_themes(
        self,
        factor_analysis: List[Dict],
        winning_patterns: List[Dict],
        requirements: List[Dict]
    ) -> List[WinTheme]:
        """
        Develop win themes based on analysis.

        v4.0: Uses LLM for intelligent theme generation when available,
        falls back to template-based generation otherwise.

        Each theme should:
        - Address a primary evaluation factor
        - Be unique/differentiated
        - Have provable evidence
        """
        # Try LLM-based theme generation first
        if self.use_llm:
            llm_themes = self._generate_themes_with_llm(factor_analysis, winning_patterns, requirements)
            if llm_themes:
                return llm_themes

        # Fallback to template-based generation
        return self._generate_themes_from_templates(factor_analysis, winning_patterns, requirements)

    def _generate_themes_with_llm(
        self,
        factor_analysis: List[Dict],
        winning_patterns: List[Dict],
        requirements: List[Dict]
    ) -> Optional[List[WinTheme]]:
        """
        Generate win themes using LLM.

        v4.0: Real strategic reasoning via LLM.
        """
        system_prompt = """You are an expert federal proposal strategist with 20+ years of experience winning government contracts.

Your task is to develop win themes that will resonate with government evaluators.

For each evaluation factor, create a win theme that:
1. Directly addresses what the government cares about most
2. Differentiates from competitors without naming them
3. Is provable with evidence (past performance, case studies, metrics)
4. Uses action-oriented, benefit-focused language

Output your themes in this exact JSON format:
{
  "themes": [
    {
      "factor_name": "Technical Approach",
      "headline": "One powerful sentence that captures why we win",
      "narrative": "2-3 sentences expanding on the headline with specific benefits",
      "discriminator_claim": "Specific claim that sets us apart (with quantification if possible)",
      "discriminator_type": "technical|management|past_performance|price|innovation|risk|personnel",
      "proof_points": ["Evidence point 1", "Evidence point 2", "Evidence point 3"],
      "ghosting_angle": "Subtle language to position against competitors without naming them",
      "priority": 1
    }
  ]
}"""

        # Build the prompt with context
        factors_text = "\n".join([
            f"- {f['factor_name']} (importance: {f['importance_score']:.0%}): {f['text'][:200]}..."
            for f in factor_analysis[:5]
        ])

        patterns_text = "\n".join([
            f"- {p['pattern_name']}: {p['description']}"
            for p in winning_patterns[:3]
        ]) if winning_patterns else "No historical patterns available."

        req_sample = "\n".join([
            f"- {r.get('text', '')[:100]}..."
            for r in requirements[:10]
        ]) if requirements else "No requirements provided."

        prompt = f"""Analyze these evaluation factors and develop win themes:

EVALUATION FACTORS (from Section M):
{factors_text}

HISTORICAL WINNING PATTERNS:
{patterns_text}

SAMPLE REQUIREMENTS TO ADDRESS:
{req_sample}

Generate 3-5 win themes that will score highest with government evaluators. Focus on the most important factors first.

Return ONLY valid JSON matching the schema above."""

        response = self._call_llm(prompt, system_prompt)
        if not response:
            return None

        # Parse JSON response
        try:
            # Extract JSON from response (handle markdown code blocks)
            json_match = re.search(r'\{[\s\S]*\}', response)
            if not json_match:
                return None

            data = json.loads(json_match.group())
            themes_data = data.get("themes", [])

            themes = []
            for i, t in enumerate(themes_data):
                # Determine discriminator type
                disc_type_str = t.get("discriminator_type", "technical").lower()
                try:
                    disc_type = DiscriminatorType(disc_type_str)
                except ValueError:
                    disc_type = DiscriminatorType.TECHNICAL

                discriminator = Discriminator(
                    id=f"DISC-{i+1:02d}",
                    category=disc_type,
                    claim=t.get("discriminator_claim", ""),
                    evidence_type="case_study",
                    evidence_source=None,
                    quantified_impact=None,
                    ghosting_angle=t.get("ghosting_angle"),
                )

                theme = WinTheme(
                    id=f"THEME-{i+1:02d}",
                    theme_headline=t.get("headline", ""),
                    theme_narrative=t.get("narrative", ""),
                    discriminators=[discriminator],
                    proof_points=t.get("proof_points", []),
                    linked_eval_criteria=[t.get("factor_name", "")],
                    ghosting_language=t.get("ghosting_angle"),
                    priority=t.get("priority", i + 1),
                    confidence=0.8,
                    llm_generated=True,
                )
                themes.append(theme)

            return themes if themes else None

        except (json.JSONDecodeError, KeyError, TypeError) as e:
            print(f"Failed to parse LLM theme response: {e}")
            return None

    def _generate_themes_from_templates(
        self,
        factor_analysis: List[Dict],
        winning_patterns: List[Dict],
        requirements: List[Dict]
    ) -> List[WinTheme]:
        """
        Template-based theme generation (fallback when LLM unavailable).
        """
        themes = []

        for i, factor in enumerate(factor_analysis[:5]):
            factor_name = factor["factor_name"]

            # Find relevant pattern
            relevant_pattern = next(
                (p for p in winning_patterns
                 if any(kw in p["pattern_name"].lower()
                       for kw in factor_name.lower().split())),
                None
            )

            # Generate theme headline
            headline = self._generate_theme_text(factor, relevant_pattern)

            # Generate discriminator
            disc_claim = self._generate_discriminator_claim(factor)
            disc_type = self._infer_discriminator_type(factor_name)

            discriminator = Discriminator(
                id=f"DISC-{i+1:02d}",
                category=disc_type,
                claim=disc_claim,
                evidence_type="case_study",
                evidence_source=None,
                quantified_impact=None,
                ghosting_angle=None,
            )

            theme = WinTheme(
                id=f"THEME-{i+1:02d}",
                theme_headline=headline,
                theme_narrative=f"Our approach to {factor_name.lower()} demonstrates proven capabilities that directly address your mission requirements.",
                discriminators=[discriminator],
                proof_points=self._generate_proof_points(factor, requirements),
                linked_eval_criteria=[factor["criterion_id"]] if factor.get("criterion_id") else [],
                ghosting_language=None,
                priority=i + 1,
                confidence=factor["importance_score"],
                llm_generated=False,
            )
            themes.append(theme)

        return themes

    def _infer_discriminator_type(self, factor_name: str) -> DiscriminatorType:
        """Infer discriminator type from factor name"""
        factor_lower = factor_name.lower()
        if "technical" in factor_lower:
            return DiscriminatorType.TECHNICAL
        elif "management" in factor_lower:
            return DiscriminatorType.MANAGEMENT
        elif "past" in factor_lower or "performance" in factor_lower:
            return DiscriminatorType.PAST_PERFORMANCE
        elif "price" in factor_lower or "cost" in factor_lower:
            return DiscriminatorType.PRICE
        elif "staff" in factor_lower or "personnel" in factor_lower:
            return DiscriminatorType.PERSONNEL
        elif "risk" in factor_lower:
            return DiscriminatorType.RISK
        else:
            return DiscriminatorType.TECHNICAL

    def _generate_discriminator_claim(self, factor: Dict) -> str:
        """Generate discriminator claim (legacy compatibility)"""
        return self._generate_discriminator(factor)
    
    def _generate_theme_text(
        self, 
        factor: Dict, 
        pattern: Optional[Dict]
    ) -> str:
        """Generate the headline theme text"""
        factor_name = factor["factor_name"]
        
        # Theme templates by factor type
        templates = {
            "Technical Approach": [
                "Proven Technical Excellence Delivered Through Innovation",
                "Mission-Focused Technical Solutions Built on Experience",
                "Accelerated Results Through Proven Methodology"
            ],
            "Management Approach": [
                "Seamless Transition Through Experienced Leadership",
                "Risk-Mitigated Execution From Day One",
                "Proven Management Framework Ensures Success"
            ],
            "Past Performance": [
                "Demonstrated Success in Identical Mission Environments",
                "Track Record of Excellence with Government Customers",
                "Proven Past Performance Directly Relevant to Your Mission"
            ],
            "Price/Cost": [
                "Best Value Through Operational Efficiency",
                "Cost-Effective Solutions Without Compromising Quality",
                "Transparent Pricing with Maximum ROI"
            ],
            "Staffing/Key Personnel": [
                "Mission-Ready Team with Proven Expertise",
                "Committed Key Personnel with Direct Experience",
                "Expert Staff Immediately Available for Transition"
            ]
        }
        
        # Get appropriate template
        if factor_name in templates:
            return templates[factor_name][0]
        else:
            return f"Excellence in {factor_name} Through Proven Capabilities"
    
    def _generate_discriminator(self, factor: Dict) -> str:
        """Generate the unique discriminator"""
        factor_name = factor["factor_name"]
        
        discriminators = {
            "Technical Approach": "Our proprietary methodology reduces implementation risk by 40%",
            "Management Approach": "Zero-defect transition record across 15 similar contracts",
            "Past Performance": "Direct relevant experience with same agency mission",
            "Price/Cost": "Lean operations model delivers 15% cost savings",
            "Staffing/Key Personnel": "All key personnel have current security clearances"
        }
        
        return discriminators.get(factor_name, f"Differentiated approach to {factor_name}")
    
    def _generate_proof_points(
        self, 
        factor: Dict, 
        requirements: List[Dict]
    ) -> List[str]:
        """Generate evidence points to support the theme"""
        proof_points = []
        
        # Get key phrases from the factor
        key_phrases = factor.get("key_phrases", [])
        
        # Match requirements that could provide evidence
        for req in requirements[:20]:  # Sample first 20
            req_keywords = req.get("keywords", [])
            if any(kp.lower() in " ".join(req_keywords).lower() for kp in key_phrases):
                proof_points.append(f"Directly addresses: {req.get('section_ref', 'requirement')}")
        
        # Add generic proof point templates
        factor_name = factor["factor_name"]
        
        if factor_name == "Technical Approach":
            proof_points.extend([
                "Technical approach diagram in Section X.X",
                "Innovation feature addresses Section C requirement",
                "Risk mitigation matrix demonstrates proactive planning"
            ])
        elif factor_name == "Past Performance":
            proof_points.extend([
                "Contract ABC123 - Same scope, same customer",
                "Quantified metrics: 99.9% uptime achieved",
                "CPARs rating: Exceptional"
            ])
        
        return proof_points[:5]  # Limit to 5 proof points per theme
    
    def _analyze_competitors(
        self, 
        competitor_data: Dict,
        factor_analysis: List[Dict]
    ) -> Dict[str, Any]:
        """Analyze competitive landscape"""
        analysis = {
            "competitors_identified": [],
            "competitive_gaps": [],
            "win_probability_factors": []
        }
        
        # If competitor data provided
        if competitor_data.get("competitors"):
            for comp in competitor_data["competitors"]:
                profile = CompetitorProfile(
                    name=comp.get("name", "Unknown"),
                    strengths=comp.get("strengths", []),
                    weaknesses=comp.get("weaknesses", []),
                    likely_themes=self._predict_competitor_themes(comp, factor_analysis),
                    ghosting_opportunities=self._identify_ghosting_ops(comp)
                )
                analysis["competitors_identified"].append({
                    "name": profile.name,
                    "strengths": profile.strengths,
                    "weaknesses": profile.weaknesses,
                    "likely_themes": profile.likely_themes,
                    "ghosting_opportunities": profile.ghosting_opportunities
                })
        
        return analysis
    
    def _predict_competitor_themes(
        self, 
        competitor: Dict, 
        factors: List[Dict]
    ) -> List[str]:
        """Predict what themes a competitor will likely use"""
        themes = []
        
        strengths = competitor.get("strengths", [])
        
        if "incumbent" in " ".join(strengths).lower():
            themes.append("Leverage existing knowledge and relationships")
        if "large" in " ".join(strengths).lower():
            themes.append("Emphasize resources and stability")
        if "technical" in " ".join(strengths).lower():
            themes.append("Lead with technical capabilities")
        
        return themes
    
    def _identify_ghosting_ops(self, competitor: Dict) -> List[str]:
        """Identify opportunities to ghost (de-position) competitor"""
        opportunities = []
        
        weaknesses = competitor.get("weaknesses", [])
        
        for weakness in weaknesses:
            weakness_lower = weakness.lower()
            
            if "transition" in weakness_lower or "new" in weakness_lower:
                opportunities.append(
                    "Emphasize our proven transition capability and low risk"
                )
            if "size" in weakness_lower or "small" in weakness_lower:
                opportunities.append(
                    "Highlight our scalable resources and financial stability"
                )
            if "experience" in weakness_lower:
                opportunities.append(
                    "Demonstrate directly relevant past performance"
                )
        
        return opportunities
    
    def _generate_ghosting(
        self, 
        theme: WinTheme, 
        competitor_analysis: Dict
    ) -> Optional[str]:
        """Generate ghosting language for a theme"""
        # Generic ghosting that doesn't name competitors but addresses gaps
        ghosting_templates = {
            "Technical Approach": 
                "Unlike generic approaches, our methodology is specifically "
                "designed for government environments with built-in security.",
            "Management Approach":
                "Our management team brings hands-on agency experience, "
                "not just theoretical knowledge of government operations.",
            "Past Performance":
                "We offer directly relevant recent past performance, "
                "not outdated or tangentially related contract experience.",
            "Price/Cost":
                "Our pricing reflects realistic staffing and efficient operations, "
                "not underbidding that leads to performance issues."
        }
        
        # Match theme to ghosting
        for factor, ghosting in ghosting_templates.items():
            if factor.lower() in theme.theme_text.lower():
                return ghosting
        
        return None
    
    def _create_annotated_outline(
        self,
        win_themes: List[WinTheme],
        requirements: List[Dict],
        instructions: List[Dict]
    ) -> Dict[str, Any]:
        """
        Create the annotated outline (storyboard)
        
        Maps sections to requirements, themes, and page allocations
        """
        # Extract page limits from instructions
        total_pages = 100  # Default
        for inst in instructions:
            if inst.get("page_limit"):
                total_pages = inst["page_limit"]
                break
        
        # Standard federal proposal structure
        outline = {
            "total_page_limit": total_pages,
            "volumes": {
                "volume_1_technical": {
                    "title": "Technical Volume",
                    "page_allocation": int(total_pages * 0.5),
                    "sections": []
                },
                "volume_2_management": {
                    "title": "Management Volume", 
                    "page_allocation": int(total_pages * 0.25),
                    "sections": []
                },
                "volume_3_past_performance": {
                    "title": "Past Performance Volume",
                    "page_allocation": int(total_pages * 0.15),
                    "sections": []
                },
                "volume_4_pricing": {
                    "title": "Pricing Volume",
                    "page_allocation": int(total_pages * 0.1),
                    "sections": []
                }
            }
        }
        
        # Populate technical volume sections with themes
        tech_themes = [t for t in win_themes if "technical" in t.theme_text.lower()]
        for i, theme in enumerate(tech_themes):
            outline["volumes"]["volume_1_technical"]["sections"].append({
                "section_number": f"1.{i+1}",
                "title": f"Technical Approach - {theme.discriminator[:50]}",
                "win_theme": theme.theme_text,
                "discriminator": theme.discriminator,
                "page_allocation": 10,
                "linked_requirements": theme.linked_criteria,
                "proof_points": theme.proof_points
            })
        
        # Add default sections if no themes
        if not outline["volumes"]["volume_1_technical"]["sections"]:
            outline["volumes"]["volume_1_technical"]["sections"] = [
                {"section_number": "1.1", "title": "Technical Approach Overview", "page_allocation": 15},
                {"section_number": "1.2", "title": "Methodology", "page_allocation": 20},
                {"section_number": "1.3", "title": "Innovation", "page_allocation": 10},
                {"section_number": "1.4", "title": "Risk Management", "page_allocation": 5}
            ]
        
        # Management volume
        outline["volumes"]["volume_2_management"]["sections"] = [
            {"section_number": "2.1", "title": "Management Approach", "page_allocation": 8},
            {"section_number": "2.2", "title": "Transition Plan", "page_allocation": 7},
            {"section_number": "2.3", "title": "Quality Assurance", "page_allocation": 5},
            {"section_number": "2.4", "title": "Staffing Plan", "page_allocation": 5}
        ]
        
        # Past Performance volume
        outline["volumes"]["volume_3_past_performance"]["sections"] = [
            {"section_number": "3.1", "title": "Relevant Contract 1", "page_allocation": 5},
            {"section_number": "3.2", "title": "Relevant Contract 2", "page_allocation": 5},
            {"section_number": "3.3", "title": "Relevant Contract 3", "page_allocation": 5}
        ]
        
        return outline
    
    def _determine_primary_framework(self, factor_analysis: List[Dict]) -> str:
        """Determine the primary strategy framework based on factors"""
        if not factor_analysis:
            return StrategyFramework.TECHNICAL_EXCELLENCE.value
        
        top_factor = factor_analysis[0]["factor_name"]
        
        framework_map = {
            "Technical Approach": StrategyFramework.TECHNICAL_EXCELLENCE,
            "Past Performance": StrategyFramework.PAST_PERFORMANCE,
            "Price/Cost": StrategyFramework.PRICE_TO_WIN,
            "Management Approach": StrategyFramework.RISK_MITIGATION,
        }
        
        return framework_map.get(top_factor, StrategyFramework.TECHNICAL_EXCELLENCE).value
    
    def _win_theme_to_dict(self, theme: WinTheme) -> Dict[str, Any]:
        """Convert WinTheme to dictionary for state storage"""
        return {
            "id": theme.id,
            # v4.0 fields
            "theme_headline": theme.theme_headline,
            "theme_narrative": theme.theme_narrative,
            "discriminators": [
                {
                    "id": d.id,
                    "category": d.category.value,
                    "claim": d.claim,
                    "evidence_type": d.evidence_type,
                    "evidence_source": d.evidence_source,
                    "quantified_impact": d.quantified_impact,
                    "ghosting_angle": d.ghosting_angle,
                }
                for d in theme.discriminators
            ],
            "proof_points": theme.proof_points,
            "linked_eval_criteria": theme.linked_eval_criteria,
            "ghosting_language": theme.ghosting_language,
            "page_allocation_percent": theme.page_allocation_percent,
            "priority": theme.priority,
            "confidence": theme.confidence,
            "llm_generated": theme.llm_generated,
            # Legacy compatibility
            "theme_text": theme.theme_text,
            "discriminator": theme.discriminator,
            "linked_criteria": theme.linked_criteria,
        }


def create_strategy_agent(
    llm_client: Optional[Any] = None,
    past_performance_store: Optional[Any] = None,
    llm_provider: str = "gemini",
    use_llm: bool = True
) -> StrategyAgent:
    """
    Factory function to create a Strategy Agent.

    v4.0: Now supports LLM provider configuration.

    Args:
        llm_client: Legacy LLM client (optional)
        past_performance_store: Vector store for past proposals
        llm_provider: LLM provider ("gemini", "anthropic", "openai")
        use_llm: Enable LLM-based theme generation

    Returns:
        Configured StrategyAgent instance
    """
    return StrategyAgent(
        llm_client=llm_client,
        past_performance_store=past_performance_store,
        llm_provider=llm_provider,
        use_llm=use_llm,
    )
